# 13.6.1　使用请求队列

块设备驱动在使用请求队列的场景下，会用blk_init_queue（）初始化request_queue，而该函数的第一个参数就是请求处理函数的指针。request_queue会作为参数传递给我们在调用blk_init_queue（）时指定的请求处理函数，块设备驱动请求处理函数的原型为：

```
static void xxx_req(struct request_queue *q)
```

这个函数不能由驱动自己调用，只有当内核认为是时候让驱动处理对设备的读写等操作时，它才调用这个函数。该函数的主要工作就是发起与request对应的块设备I/O动作（但是具体的I/O工作不一定要在该函数内同步完成）。代码清单13.9给出了一个简单的请求处理函数的例子，它来源于drivers/memstick/core/ms_block.c。

代码清单13.9　块设备驱动请求函数例程

```
 1static void msb_submit_req(struct request_queue *q)
 2{
 3       struct memstick_dev *card = q->queuedata;
 4       struct msb_data *msb = memstick_get_drvdata(card);
 5       struct request *req = NULL;
 6
 7       dbg_verbose("Submit request");
 8
 9       if (msb->card_dead) {
10              dbg("Refusing requests on removed card");
11
12              WARN_ON(!msb->io_queue_stopped);
13
14              while ((req = blk_fetch_request(q)) != NULL)
15                      __blk_end_request_all(req, -ENODEV);
16              return;
17       }
18
19       if (msb->req)
20               return;
21
22       if (!msb->io_queue_stopped)
23              queue_work(msb->io_queue, &msb->io_work);
24}
```

上述代码第14行使用blk_fetch_request（）获得队列中第一个未完成的请求，由于msb->card_dead成立，实际上我们处理不了该请求，所以就直接通过__blk_end_request_all（req，-ENODEV）返回错误了

正常的情况下，通过queue_work（msb->io_queue，&msb->io_work）启动工作队列执行msb_io_work（struct work_struct*work）这个函数，它的原型如代码清单13.10所示。

代码清单13.10　msb_io_work（）完成请求处理

```
 1static void msb_io_work(struct work_struct *work)
 2{
 3       struct msb_data *msb = container_of(work, struct msb_data, io_work);
 4       int page, error, len;
 5       sector_t lba;
 6       unsigned long flags;
 7       struct scatterlist *sg = msb->prealloc_sg;
 8
 9       dbg_verbose("IO: work started");
10
11       while (1) {
12               spin_lock_irqsave(&msb->q_lock, flags);
13
14               if (msb->need_flush_cache) {
15                     msb->need_flush_cache = false;
16                     spin_unlock_irqrestore(&msb->q_lock, flags);
17                     msb_cache_flush(msb);
18                     continue;
19               }
20
21               if (!msb->req) {
22                     msb->req = blk_fetch_request(msb->queue);
23                     if (!msb->req) {
24                             dbg_verbose("IO: no more requests exiting");
25                             spin_unlock_irqrestore(&msb->q_lock, flags);
26                             return;
27                     }
28               }
29
30               spin_unlock_irqrestore(&msb->q_lock, flags);
31
32               /* If card was removed meanwhile */
33               if (!msb->req)
34                     return;
35
36               /* process the request */
37               dbg_verbose("IO: processing new request");
38               blk_rq_map_sg(msb->queue, msb->req, sg);
39
40               lba = blk_rq_pos(msb->req);
41
42               sector_div(lba, msb->page_size / 512);
43               page = do_div(lba, msb->pages_in_block);
44
45               if (rq_data_dir(msb->req) == READ)
46                       error = msb_do_read_request(msb, lba, page, sg,
47                               blk_rq_bytes(msb->req), &len);
48               else
49                       error = msb_do_write_request(msb, lba, page, sg,
50                               blk_rq_bytes(msb->req), &len);
51
52               spin_lock_irqsave(&msb->q_lock, flags);
53
54               if (len)
55                       if (!__blk_end_request(msb->req, 0, len))
56                               msb->req = NULL;
57
58               if (error && msb->req) {
59                     dbg_verbose("IO: ending one sector of the request with error");
60                     if (!__blk_end_request(msb->req, error, msb->page_size))
61                             msb->req = NULL;
62               }
63
64               if (msb->req)
65                     dbg_verbose("IO: request still pending");
66
67               spin_unlock_irqrestore(&msb->q_lock, flags);
68       }
69}
```

在读写无错的情况下，第55行调用的__blk_end_request（msb->req，0，len）实际上告诉了上层该请求处理完成。如果读写有错，则调用__blk_end_request（msb->req，error，msb->page_size），把出错原因作为第2个参数传入上层。

第38行调用的blk_rq_map_sg（）函数实现于block/blk-merge.c文件。代码清单13.11列出了该函数的实现中比较精华的部分，它通过rq_for_each_bio（）、bio_for_each_segment（）来遍历所有的bio，以及所有的片段，将所有与某请求相关的页组成一个scatter/gather的列表。

代码清单13.11　blk_rq_map_sg（）函数

```
 1int blk_rq_map_sg(struct request_queue *q, struct request *rq,
 2                  struct scatterlist *sglist)
 3{
 4      struct scatterlist *sg = NULL;
 5      int nsegs = 0;
 6
 7      if (rq->bio)
 8              nsegs = __blk_bios_map_sg(q, rq->bio, sglist, &sg);
 9      ...
10}
11
12static int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,
13                             struct scatterlist *sglist,
14                             struct scatterlist **sg)
15{
16        struct bio_vec bvec, bvprv = { NULL };
17        struct bvec_iter iter;
18        int nsegs, cluster;
19
20      nsegs = 0;
21      cluster = blk_queue_cluster(q);
22      ...
23      for_each_bio(bio)
24              bio_for_each_segment(bvec, bio, iter)
25                      __blk_segment_map_sg(q, &bvec, sglist, &bvprv, sg,
26                                           &nsegs, &cluster);
27
28      return nsegs;
29}
30
31static inline void
32__blk_segment_map_sg(struct request_queue *q, struct bio_vec *bvec,
33                     struct scatterlist *sglist, struct bio_vec *bvprv,
34                     struct scatterlist **sg, int *nsegs, int *cluster)
35{
36
37      int nbytes = bvec->bv_len;
38
39      if (*sg && *cluster) {
40              if ((*sg)->length + nbytes > queue_max_segment_size(q))
41                      goto new_segment;
42
43              if (!BIOVEC_PHYS_MERGEABLE(bvprv, bvec))
44                      goto new_segment;
45              if (!BIOVEC_SEG_BOUNDARY(q, bvprv, bvec))
46                      goto new_segment;
47
48              (*sg)->length += nbytes;
49      } else {
50new_segment:
51                if (!*sg)
52                      *sg = sglist;
53                else {
54                      /*
55                       * If the driver previously mapped a shorter
56                       * list, we could see a termination bit
57                       * prematurely unless it fully inits the sg
58                       * table on each mapping. We KNOW that there
59                       * must be more entries here or the driver
60                       * would be buggy, so force clear the
61                       * termination bit to avoid doing a full
62                       * sg_init_table() in drivers for each command.
63                       */
64                      sg_unmark_end(*sg);
65                      *sg = sg_next(*sg);
66                }
67
68                sg_set_page(*sg, bvec->bv_page, nbytes, bvec->bv_offset);
69                (*nsegs)++;
70        }
71        *bvprv = *bvec;
```

一般情况下，若外设支持scatter/gather模式的DMA操作，紧接着，它就会执行pci_map_sg（）或者dma_map_sg（）来进行上述scatter/gather列表的DMA映射了，之后进行硬件的访问。

# 13.6.2　不使用请求队列

使用请求队列对于一个机械磁盘设备而言的确有助于提高系统的性能，但是对于RAMDISK、ZRAM（Compressed RAM Block Device）等完全可真正随机访问的设备而言，无法从高级的请求队列逻辑中获益。对于这些设备，块层支持“无队列”的操作模式，为使用这个模式，驱动必须提供一个“制造请求”函数，而不是一个请求处理函数，“制造请求”函数的原型为：

```
static void xxx_make_request(struct request_queue *queue, struct bio *bio);
```

块设备驱动初始化的时候不再调用blk_init_queue（），而是调用blk_alloc_queue（）和blk_queue_make_request（），xxx_make_request则会成为blk_queue_make_request（）的第2个参数。

xxx_make_request（）函数的第一个参数仍然是“请求队列”，但是这个“请求队列”实际不包含任何请求，因为块层没有必要将bio调整为请求。因此，“制造请求”函数的主要参数是bio结构体。代码清单13.12所示为一个“制造请求”函数的例子，它取材于drivers/block/zram/zram_drv.c。

代码清单13.12　“制造请求”函数例程

```
 1static void zram_make_request(struct request_queue *queue, struct bio *bio)
 2{
 3        ...
 4        __zram_make_request(zram, bio);
 5        ...
 6}
 7
 8static void __zram_make_request(struct zram *zram, struct bio *bio)
 9{
10        int offset;
11        u32index;
12        struct bio_vec bvec;
13        struct bvec_iter iter;
14
15        index = bio->bi_iter.bi_sector >> SECTORS_PER_PAGE_SHIFT;
16        offset = (bio->bi_iter.bi_sector &
17                  (SECTORS_PER_PAGE - 1)) << SECTOR_SHIFT;
18
19        if (unlikely(bio->bi_rw & REQ_DISCARD)) {
20                zram_bio_discard(zram, index, offset, bio);
21                bio_endio(bio, 0);
22                return;
23        }
24
25        bio_for_each_segment(bvec, bio, iter) {
26                int max_transfer_size = PAGE_SIZE - offset;
27
28                if (bvec.bv_len > max_transfer_size) {
29                        /*
30                         * zram_bvec_rw() can only make operation on a single
31                         * zram page. Split the bio vector.
32                         */
33                        struct bio_vec bv;
34
35                        bv.bv_page = bvec.bv_page;
36                        bv.bv_len = max_transfer_size;
37                        bv.bv_offset = bvec.bv_offset;
38
39                        if (zram_bvec_rw(zram, &bv, index, offset, bio) < 0)
40                                goto out;
41
42                        bv.bv_len = bvec.bv_len - max_transfer_size;
43                        bv.bv_offset += max_transfer_size;
44                        if (zram_bvec_rw(zram, &bv, index + 1, 0, bio) < 0)
45                                goto out;
46                } else
47                        if (zram_bvec_rw(zram, &bvec, index, offset, bio) < 0)
48                                goto out;
49
50                update_position(&index, &offset, &bvec);
51        }
52
53        set_bit(BIO_UPTODATE, &bio->bi_flags);
54        bio_endio(bio, 0);
55        return;
56
57out:
58        bio_io_error(bio);
59}
```

上述代码通过bio_for_each_segment（）迭代bio中的每个segement，最终调用zram_bvec_rw（）完成内存的压缩、解压、读取和写入。

ZRAM是Linux的一种内存优化技术，它划定一片内存区域作为SWAP的交换分区，但是它本身具备自动压缩功能，从而可以达到辅助Linux匿名页的交换效果，变相“增大”了内存。